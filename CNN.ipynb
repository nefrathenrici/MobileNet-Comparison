{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Resnet-18 Model\n",
    "\n",
    "by Alvin Zheng and Nat Efrat-Henrici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is built to run on the tiny-imagenet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "#googlenet = models.googlenet(pretrained=True)\n",
    "network = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch as torch\n",
    "import numpy as np\n",
    "import os\n",
    "    \n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gzip\n",
    "import matplotlib.image as mpimg \n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "import glob\n",
    "import torchvision.transforms as transforms\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "datapath = \"./tiny-imagenet-200/\"\n",
    "trainpath = datapath + \"train/\"\n",
    "classIDs = os.listdir(trainpath)\n",
    "classIDDict = {}\n",
    "for i in range(len(classIDs)):\n",
    "    classIDDict[classIDs[i]] = i\n",
    "\n",
    "classCount = len(classIDs)\n",
    "sourceDim = 64\n",
    "targetDim = 224\n",
    "channels = 3\n",
    "\n",
    "trainPerClass = 450   #500 original\n",
    "testPerClass = 10\n",
    "validPerClass = 20\n",
    "\n",
    "imageCountTr = classCount  * trainPerClass\n",
    "imageCountTs = classCount  * testPerClass\n",
    "imageCountV = classCount  * validPerClass\n",
    "\n",
    "def stackImage(image):\n",
    "    return np.stack((im,)*channels, axis=-1)\n",
    "\n",
    "def processImage(image):\n",
    "    if (len(image.shape) < channels):\n",
    "        image = stackImage(image)\n",
    "    asfloat = image.astype('float32')\n",
    "    resized = cv2.resize(asfloat, dsize=(targetDim,targetDim), interpolation=cv2.INTER_CUBIC)\n",
    "    rolled = np.rollaxis(resized, 2, 0)\n",
    "    tensor = torch.tensor(rolled)\n",
    "    normalized = normalize(tensor)\n",
    "    numpied = normalized.numpy()[:,:,:]\n",
    "    return numpied\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training, test, validation data\n",
    "imagesTr = np.zeros((imageCountTr, channels, targetDim, targetDim), dtype=\"float32\")\n",
    "labelsTr = np.zeros(imageCountTr, dtype=int)\n",
    "\n",
    "imagesTs = np.zeros((imageCountTs, channels, targetDim, targetDim), dtype=\"float32\")\n",
    "labelsTs = np.zeros(imageCountTs, dtype=int)\n",
    "\n",
    "imagesV = np.zeros((imageCountV, channels, targetDim, targetDim), dtype=\"float32\")\n",
    "labelsV = np.zeros(imageCountV, dtype=int)\n",
    "\n",
    "dataset = [(imagesTr, labelsTr), (imagesTs, labelsTs), (imagesV, labelsV)]\n",
    "\n",
    "for i in range(len(classIDs)):\n",
    "    fileNames = glob.glob(trainpath+classIDs[i]+\"/images/*.JPEG\")\n",
    "    j = 0\n",
    "    for s in dataset:\n",
    "        length = int(len(s[0]) / classCount)\n",
    "        for k in range(length):\n",
    "            classIndex = j + k\n",
    "            setIndex = i * length + k\n",
    "            im = mpimg.imread(fileNames[classIndex].replace(\"\\\\\", \"/\"))\n",
    "            im = processImage(im)\n",
    "            label = i\n",
    "            s[0][setIndex] = im\n",
    "            s[1][setIndex] = label \n",
    "        j += length\n",
    "    print('Class: ' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesTr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#validation data\n",
    "imageCountV = 10000\n",
    "imagesV = np.zeros((imageCountV, channels, targetDim, targetDim), dtype=\"float32\")\n",
    "labelsV = np.zeros(imageCountV, dtype=int)\n",
    "pathV = datapath + \"val/\"\n",
    "fileNamesV = glob.glob(pathV + \"images/*.JPEG\")\n",
    "fpV = open(pathV + \"val_annotations.txt\", \"r\")\n",
    "for i in range(len(fileNamesV)):\n",
    "    newlineV = fpV.readline()\n",
    "    label = newlineV.split()[1]\n",
    "    filePath = fileNamesV[i].replace(\"\\\\\", \"/\")\n",
    "    imNum = int(filePath.split(\"/\")[-1][4:-5])\n",
    "    im = mpimg.imread(filePath)\n",
    "    im = processImage(im)\n",
    "    imagesV[i] = im\n",
    "    labelsV[i] = classIDDict[label]\n",
    "\n",
    "fpV.close()\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = imagesTr, labelsTr\n",
    "x_valid, y_valid = imagesV, labelsV\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.as_tensor, (x_train, y_train, x_valid, y_valid)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    dev = \"cuda:0\" \n",
    "    device = torch.device(dev)\n",
    "    network.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(x_train, 'x_train.pt')\n",
    "torch.save(y_train, 'y_train.pt')\n",
    "torch.save(x_valid, 'x_valid.pt')\n",
    "torch.save(y_valid, 'y_valid.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "im = mpimg.imread('blah.jpg')\n",
    "arr = np.zeros((2,3,224,224), dtype='float32')\n",
    "res = cv2.resize(im, dsize=(224,224), interpolation=cv2.INTER_CUBIC)\n",
    "plt.imshow(res, interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "im2 = processImage(im)\n",
    "im2.shape\n",
    "type(im2)\n",
    "#https://docs.fast.ai/vision.data#ImageDataLoaders.from_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "valid_ds = TensorDataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))\n",
    "            \n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "def preprocess1(x):\n",
    "    return x.view(-1, channels, targetDim, targetDim)\n",
    "\n",
    "def preprocess2(x, y):\n",
    "    return x.view(-1, channels, targetDim, targetDim), y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define log softmax and our model output\n",
    "\n",
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "# negative loss likelihood (equivalent to cross entropy)\n",
    "def nll(inp, target):\n",
    "    return -inp[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check our loss and accuracy after one forward pass on a batch size of 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    yb = yb.long()\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "                \n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in valid_dl:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                val_loss, nums = loss_batch(model, loss_func, xb, yb)\n",
    "                epoch_val_loss +=  val_loss\n",
    "            epoch_val_loss /= nums\n",
    "        print(epoch, epoch_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "lr = 2.85e-3  # learning rate\n",
    "epochs = 5  # how many epochs to train for\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = WrappedDataLoader(train_dl, preprocess2)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess2)\n",
    "'''\n",
    "model = nn.Sequential(\n",
    "    Lambda(preprocess1),\n",
    "    nn.Conv2d(channels, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(256, imageClassCount, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(4),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")'''\n",
    "\n",
    "opt = optim.SGD(network.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(epochs, network, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adde(x, y):\n",
    "    print(x+y)\n",
    "def adde(x):\n",
    "    print(x)\n",
    "\n",
    "adde(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(googlenet, './googlenet.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_lr_finder import LRFinder\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(network.parameters(), lr=1e-7, weight_decay=1e-2)\n",
    "lr_finder = LRFinder(network, optimizer, criterion, device=dev)\n",
    "lr_finder.range_test(train_dl, end_lr=100, num_iter=100)\n",
    "lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "lr_finder.reset() # to reset the model and optimizer to their initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network = torch.load('./resnet.pt')\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "val_acc = 0\n",
    "counter = 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in valid_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        out = network(xb)\n",
    "        val_acc += accuracy(out, yb)\n",
    "        counter += 1\n",
    "print(val_acc/counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classIDDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}